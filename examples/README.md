## Files

-   **train.py**: This file contains the model definition (currently 3 GCN layers with 128 hidden dimension size), data loading, training loop, and other essential components for training a GNN.
This script has various command-line arguments, including:

  * `--data_dir`: Specifies the directory containing the dataset, which can be either in unpartitioned or partitioned format.

  * `--G_intra_r` (optional): Specifies the X dimension of the 3D parallelism configuration (default: 1).

  * `--G_intra_c` (optional): Specifies the Y dimension of the 3D parallelism configuration (default: 1).

  * `--G_intra_d` (optional): Specifies the Z dimension of the 3D parallelism configuration (default: 1).

  * `--gpus_per_node` (optional): Specifies the number of GPUs available on each node (default: inferred from PyTorch).

  * `--num_epochs` (optional): Determines the number of training epochs (default: 2).

  * `--block_aggregation` (optional): If set (`--block_aggregation`), performs aggregation (both the SpMM and all-reduce) on 1D row blocks instead of the full adjacency matrix, which can help to reduce variability in some cases..

  * `--overlap_aggregation` (optional): If set (`--overlap_aggregation`), enables overlapping of SpMM and all-reduce in the aggregation (default: False). This can only be applied if block_aggregation is.

  * `--seed` (optional): Sets the random seed for all GPUs. If not provided, the default seed of 0 is used.

-   **run_4.sh**: This is an example shell script for Perlmutter, demonstrating how to run a Plexus-parallelized GNN on 4 GPUs.  It includes placeholders that should be replaced with appropriate values for specific experiments, such as dataset path, output directory, etc. The script can be adapted to run on different numbers of GPUs and with different datasets.

    For example, the script can be launched using:
    ```bash
    sbatch run_4.sh 1 1 4 0
    ```
    This would execute the training with a 3D parallelism configuration of (X, Y, Z) = (1, 1, 4) for trial number 0. The trial number is often used to differentiate output files from multiple runs.

-   **get_rank.sh**: This shell script is used to set the ranks for the GPUs involved in the distributed training process. It also limits the core dump file size to 0.

-   **parse_results.py**: This Python script contains the `process_log_file` function, which can be used to parse the timing results from the output log file generated by a training run. 

-   `export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"`: This can be set if there are warnings about fragmentation which can cause GPU OOM issues. 
