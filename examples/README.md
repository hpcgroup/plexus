## Files

-   **train.py**: This file contains the model definition, data loading, training loop, and other essential components for training a GNN.

-   **run_4.sh**: This is an example shell script for Perlmutter, demonstrating how to run a Plexus-parallelized GNN on 4 GPUs.  It includes placeholders that should be replaced with appropriate values for specific experiments, such as dataset path, output directory, etc. The script can be adapted to run on different numbers of GPUs and with different datasets.

    For example, the script can be launched using:
    ```bash
    sbatch run_4.sh 1 1 4 0
    ```
    This would execute the training with a 3D parallelism configuration of (X, Y, Z) = (1, 1, 4) for trial number 0. The trial number is often used to differentiate output files from multiple runs.

-   **get_rank.sh**: This shell script is used to set the ranks for the GPUs involved in the distributed training process. It also limits the core dump file size to 0.

-   **parse_results.py**: This Python script contains the `process_log_file` function, which can be used to parse the timing results from the output log file generated by a training run. 

-   `export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"`: This can be set if there are warnings about fragmentation which can cause GPU OOM issues. 
